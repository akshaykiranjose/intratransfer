{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91664416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6b88f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cf534c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, dropout_p = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_size = hidden_size, hidden_size = hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"x : [N, L, 2]\"\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        encoder_outputs, encoder_hidden = self.lstm(embedded)\n",
    "        return  encoder_outputs, encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19d815d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, dropout_p = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_size = hidden_size, hidden_size = hidden_size, batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.first_decoder_input = torch.rand((BATCH_SIZE, 1, hidden_size))\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "      \n",
    "    def attn_scores(encoder_outputs, decoder_hidden):\n",
    "        return self.V(F.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden))).squeeze(-1)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor, target_lengths):\n",
    "        '''\n",
    "        encoder_hidden = [N, 1, H] \n",
    "        encoder_outputs = [N, L, H] \n",
    "        decoder_target = [N, L+1]\n",
    "        target_tensor = [N, L, 2]\n",
    "        target_lengths = [N, 1]\n",
    "        '''\n",
    "        \n",
    "        batch_size, max_len, _ = encoder_outputs.size()\n",
    "        encoder_outputs = torch.cat((self.first_decoder_input, encoder_outputs), dim=1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.rand((batch_size, 1, 2), \n",
    "                                    dtype=torch.long)\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        for i in range(max_len + 1):\n",
    "            \n",
    "            decoder_input_embed = self.dropout(self.embedding(decoder_input))\n",
    "            decoder_output, *decoder_hidden = self.lstm(decoder_input_embed, decoder_hidden)\n",
    "            \n",
    "            '''decoder_output = [N, 1, H]'''\n",
    "            raw_attn_scores = attn_scores(encoder_outputs, decoder_output)\n",
    "            '''\n",
    "            raw_attn_scores = [N, L+1]\n",
    "            mask attn_scores and softmax over only needed dictionary'''\n",
    "            ones = torch.full(size = (batch_size, max_len + 1), fill_value = 1., dtype=torch.float)\n",
    "            mask = mask.masked_fill(torch.cumsum(ones, dim=-1) > target_lengths + 1., float('-inf'))\n",
    "        \n",
    "            masked_attn_scores = raw_attn_scores * mask\n",
    "            #masked_attn_softmax_scores = F.softmax(masked_attn_scores, dim=-1)\n",
    "            \n",
    "            decoder_outputs.append(masked_attn_scores)\n",
    "            \n",
    "            decoder_input = target_tensor[:, i, :].unsqueeze(1)\n",
    "            \n",
    "        decoder_outputs = torch.concat(decoder_outputs, dim=1)\n",
    "        return decoder_outputs, \n",
    "        \n",
    "    \n",
    "    def calculate_loss(decoder_outputs, decoder_target, target_lengths):\n",
    "        loss = 0\n",
    "        b_sz, max_len = decoder_target.size()\n",
    "        for i in range(b_sz):\n",
    "            weights = torch.zeros((max_len))\n",
    "            weights[:target_lengths[i]+1] = 1.\n",
    "            for c in range(max_len):\n",
    "                loss += F.functional.cross_entropy(input = decoder_outputs[i, c, ...].unsqueeze(0), \n",
    "                                                   target= decoder_target[i, c], \n",
    "                                                   weight=weights,\n",
    "                                                   ignore_index = -1)\n",
    "        return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d9c59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e0e847b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20153eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def solve_tsp_dynamic(points):\n",
    "    def length(x_coord, y_coord):\n",
    "        return np.linalg.norm(np.asarray(x_coord) - np.asarray(y_coord))\n",
    "    #calc all lengths\n",
    "    all_distances = [[length(x,y) for y in points] for x in points]\n",
    "    #initial value - just distance from 0 to every other point + keep the track of edges\n",
    "    A = {(frozenset([0, idx+1]), idx+1): (dist, [0,idx+1]) for idx,dist in enumerate(all_distances[0][1:])}\n",
    "    cnt = len(points)\n",
    "    for m in range(2, cnt):\n",
    "        B = {}\n",
    "        for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n",
    "              for j in S - {0}:\n",
    "                B[(S, j)] = min( [(A[(S-{j},k)][0] + all_distances[k][j], A[(S-{j},k)][1] + [j]) \\\n",
    "                                  for k in S if k != 0 and k!=j])  #this will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n",
    "        A = B\n",
    "    res = min([(A[d][0] + all_distances[0][d[1]], A[d][1]) for d in iter(A)])\n",
    "    return np.asarray(res[1]) + 1 # 0 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2996710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.24654737, 0.28955304],\n",
       "        [0.81493453, 0.32096205],\n",
       "        [0.76229006, 0.5276015 ],\n",
       "        [0.3558138 , 0.6140508 ],\n",
       "        [0.3980779 , 0.78049744],\n",
       "        [0.33152915, 0.96015033],\n",
       "        [0.00903356, 0.70169921]]),\n",
       " array([[0.63434193, 0.48949929],\n",
       "        [0.22053111, 0.31208303],\n",
       "        [0.32576242, 0.91834259],\n",
       "        [0.72666392, 0.65154926],\n",
       "        [0.93390254, 0.23901986],\n",
       "        [0.68041478, 0.80601716],\n",
       "        [0.90774641, 0.09020232],\n",
       "        [0.73596878, 0.4775064 ]]),\n",
       " array([[0.0831995 , 0.54500851],\n",
       "        [0.46698088, 0.68325647],\n",
       "        [0.40556722, 0.39281098],\n",
       "        [0.67254393, 0.96984177],\n",
       "        [0.73834042, 0.9472338 ],\n",
       "        [0.25537716, 0.69658391],\n",
       "        [0.32134913, 0.55591329],\n",
       "        [0.31439819, 0.52454627],\n",
       "        [0.66510867, 0.70989666]]),\n",
       " array([[0.34096657, 0.96257525],\n",
       "        [0.20009105, 0.99308967],\n",
       "        [0.69466211, 0.78232001],\n",
       "        [0.57594894, 0.36426997],\n",
       "        [0.11037123, 0.7763809 ],\n",
       "        [0.64857357, 0.6866884 ],\n",
       "        [0.86995606, 0.88629269],\n",
       "        [0.49219901, 0.64460123],\n",
       "        [0.23007768, 0.53060379],\n",
       "        [0.71817955, 0.46488023]]),\n",
       " array([[0.64061248, 0.34363974],\n",
       "        [0.24624233, 0.1510632 ],\n",
       "        [0.02798543, 0.48280796],\n",
       "        [0.5081521 , 0.52731197],\n",
       "        [0.36743495, 0.53000656],\n",
       "        [0.16372602, 0.07916254],\n",
       "        [0.58491014, 0.85419796],\n",
       "        [0.71103979, 0.27583985],\n",
       "        [0.98986249, 0.01543299],\n",
       "        [0.2880848 , 0.50915668],\n",
       "        [0.38768341, 0.32066887]])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsp_coords = [np.random.uniform(size=[_,2]) for _ in range(7, 12)]\n",
    "tsp_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b003727",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ddf18006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3, 4, 5, 6, 7]),\n",
       " array([1, 8, 5, 7, 2, 3, 6, 4]),\n",
       " array([1, 6, 2, 4, 5, 9, 3, 8, 7]),\n",
       " array([ 1,  2,  5,  9,  8,  4, 10,  6,  3,  7]),\n",
       " array([ 1,  8,  9, 11,  2,  6,  3, 10,  5,  7,  4])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsp_tours = [solve_tsp_dynamic(c.tolist()) for c in tsp_coords]\n",
    "tsp_tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3023bee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(x) for x in tsp_tours])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "825a80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6,  7, -1, -1, -1, -1],\n",
       "       [ 1,  8,  5,  7,  2,  3,  6,  4, -1, -1, -1],\n",
       "       [ 1,  6,  2,  4,  5,  9,  3,  8,  7, -1, -1],\n",
       "       [ 1,  2,  5,  9,  8,  4, 10,  6,  3,  7, -1],\n",
       "       [ 1,  8,  9, 11,  2,  6,  3, 10,  5,  7,  4]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.pad(row, (0, max_len-len(row)), constant_values = -1.) for row in tsp_tours])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c6a840b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  5.,  6.,  7., -1., -1., -1., -1.,  0.],\n",
       "       [ 1.,  8.,  5.,  7.,  2.,  3.,  6.,  4., -1., -1., -1.,  0.],\n",
       "       [ 1.,  6.,  2.,  4.,  5.,  9.,  3.,  8.,  7., -1., -1.,  0.],\n",
       "       [ 1.,  2.,  5.,  9.,  8.,  4., 10.,  6.,  3.,  7., -1.,  0.],\n",
       "       [ 1.,  8.,  9., 11.,  2.,  6.,  3., 10.,  5.,  7.,  4.,  0.]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor = np.zeros((batch_size, max_len+1))\n",
    "target_tensor[:, :max_len] = np.array([np.pad(row, (0, max_len-len(row)), constant_values = -1.) for row in tsp_tours])\n",
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "344c64b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.24654737, 0.28955304],\n",
       "        [0.81493453, 0.32096205],\n",
       "        [0.76229006, 0.5276015 ],\n",
       "        [0.3558138 , 0.6140508 ],\n",
       "        [0.3980779 , 0.78049744],\n",
       "        [0.33152915, 0.96015033],\n",
       "        [0.00903356, 0.70169921],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.63434193, 0.48949929],\n",
       "        [0.22053111, 0.31208303],\n",
       "        [0.32576242, 0.91834259],\n",
       "        [0.72666392, 0.65154926],\n",
       "        [0.93390254, 0.23901986],\n",
       "        [0.68041478, 0.80601716],\n",
       "        [0.90774641, 0.09020232],\n",
       "        [0.73596878, 0.4775064 ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.0831995 , 0.54500851],\n",
       "        [0.46698088, 0.68325647],\n",
       "        [0.40556722, 0.39281098],\n",
       "        [0.67254393, 0.96984177],\n",
       "        [0.73834042, 0.9472338 ],\n",
       "        [0.25537716, 0.69658391],\n",
       "        [0.32134913, 0.55591329],\n",
       "        [0.31439819, 0.52454627],\n",
       "        [0.66510867, 0.70989666],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.34096657, 0.96257525],\n",
       "        [0.20009105, 0.99308967],\n",
       "        [0.69466211, 0.78232001],\n",
       "        [0.57594894, 0.36426997],\n",
       "        [0.11037123, 0.7763809 ],\n",
       "        [0.64857357, 0.6866884 ],\n",
       "        [0.86995606, 0.88629269],\n",
       "        [0.49219901, 0.64460123],\n",
       "        [0.23007768, 0.53060379],\n",
       "        [0.71817955, 0.46488023],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.64061248, 0.34363974],\n",
       "        [0.24624233, 0.1510632 ],\n",
       "        [0.02798543, 0.48280796],\n",
       "        [0.5081521 , 0.52731197],\n",
       "        [0.36743495, 0.53000656],\n",
       "        [0.16372602, 0.07916254],\n",
       "        [0.58491014, 0.85419796],\n",
       "        [0.71103979, 0.27583985],\n",
       "        [0.98986249, 0.01543299],\n",
       "        [0.2880848 , 0.50915668],\n",
       "        [0.38768341, 0.32066887]]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = np.array([np.pad(x, pad_width = ((0, max_len - len(x)), (0, 0)), constant_values = np.array([0., 0.])) for x in tsp_coords])\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "43230c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.from_numpy(input_tensor).float()\n",
    "decoder_target = torch.from_numpy(target_tensor).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "99373690",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lengths = torch.tensor([7, 8, 9, 10, 11]).unsqueeze(-1).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c38dbdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11]], dtype=torch.int32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "fbd9ec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.dtype, decoder_target.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a629674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7, -1, -1, -1, -1,  0],\n",
       "        [ 1,  8,  5,  7,  2,  3,  6,  4, -1, -1, -1,  0],\n",
       "        [ 1,  6,  2,  4,  5,  9,  3,  8,  7, -1, -1,  0],\n",
       "        [ 1,  2,  5,  9,  8,  4, 10,  6,  3,  7, -1,  0],\n",
       "        [ 1,  8,  9, 11,  2,  6,  3, 10,  5,  7,  4,  0]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d6021983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 11, 2]), torch.Size([5, 12]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, decoder_target.shape, target_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6711487e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "4eed865e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11, 128])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5b96c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 11, 128]), [torch.Size([1, 5, 128]), torch.Size([1, 5, 128])])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size(), [x.size() for x in encoder_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "edccc42c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder(encoder_outputs, encoder_hidden, input_tensor, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1770a265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0305,  0.1693,  0.1382,  0.1344,  0.1676,  0.1633,  0.1728,  0.1659,\n",
       "           0.1714,  0.1932,  0.1977,  0.1842],\n",
       "         [-0.0316,  0.1048,  0.1168,  0.1543,  0.1521,  0.1274,  0.1306,  0.1215,\n",
       "           0.0934,  0.1142,  0.1344,  0.1549],\n",
       "         [-0.2054,  0.1482,  0.1449,  0.1686,  0.1283,  0.1282,  0.1517,  0.1382,\n",
       "           0.1510,  0.1334,  0.1266,  0.1252],\n",
       "         [-0.1243,  0.1600,  0.1840,  0.1593,  0.1572,  0.1846,  0.1700,  0.1661,\n",
       "           0.1811,  0.1893,  0.1360,  0.1638],\n",
       "         [ 0.0793,  0.1350,  0.1309,  0.1629,  0.1731,  0.1909,  0.1784,  0.1733,\n",
       "           0.1375,  0.1233,  0.1836,  0.1687]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0470,  0.1556,  0.1247,  0.1211,  0.1543,  0.1502,  0.1595,  0.1526,\n",
       "           0.1582,  0.1798,  0.1843,  0.1707],\n",
       "         [-0.0209,  0.1181,  0.1301,  0.1678,  0.1655,  0.1409,  0.1441,  0.1351,\n",
       "           0.1070,  0.1278,  0.1480,  0.1685],\n",
       "         [-0.1967,  0.1607,  0.1574,  0.1814,  0.1406,  0.1412,  0.1645,  0.1507,\n",
       "           0.1638,  0.1462,  0.1392,  0.1378],\n",
       "         [-0.1355,  0.1491,  0.1732,  0.1486,  0.1465,  0.1740,  0.1595,  0.1555,\n",
       "           0.1705,  0.1789,  0.1257,  0.1535],\n",
       "         [ 0.0894,  0.1511,  0.1471,  0.1792,  0.1894,  0.2072,  0.1948,  0.1894,\n",
       "           0.1538,  0.1398,  0.2000,  0.1850]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0457,  0.1577,  0.1270,  0.1235,  0.1567,  0.1526,  0.1618,  0.1550,\n",
       "           0.1606,  0.1821,  0.1866,  0.1729],\n",
       "         [ 0.0045,  0.1446,  0.1565,  0.1942,  0.1917,  0.1669,  0.1702,  0.1612,\n",
       "           0.1333,  0.1541,  0.1743,  0.1949],\n",
       "         [-0.1873,  0.1704,  0.1671,  0.1912,  0.1505,  0.1514,  0.1745,  0.1607,\n",
       "           0.1738,  0.1562,  0.1490,  0.1476],\n",
       "         [-0.1261,  0.1602,  0.1843,  0.1599,  0.1579,  0.1853,  0.1707,  0.1670,\n",
       "           0.1819,  0.1902,  0.1373,  0.1649],\n",
       "         [ 0.0970,  0.1614,  0.1574,  0.1893,  0.1996,  0.2172,  0.2048,  0.1994,\n",
       "           0.1638,  0.1501,  0.2100,  0.1951]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0256,  0.1779,  0.1472,  0.1438,  0.1768,  0.1728,  0.1820,  0.1752,\n",
       "           0.1809,  0.2022,  0.2067,  0.1930],\n",
       "         [ 0.0051,  0.1471,  0.1591,  0.1967,  0.1942,  0.1694,  0.1727,  0.1637,\n",
       "           0.1359,  0.1566,  0.1768,  0.1974],\n",
       "         [-0.1674,  0.1898,  0.1866,  0.2107,  0.1701,  0.1711,  0.1940,  0.1803,\n",
       "           0.1933,  0.1757,  0.1685,  0.1671],\n",
       "         [-0.1209,  0.1668,  0.1908,  0.1666,  0.1646,  0.1919,  0.1774,  0.1737,\n",
       "           0.1886,  0.1969,  0.1441,  0.1716],\n",
       "         [ 0.0884,  0.1542,  0.1502,  0.1821,  0.1925,  0.2100,  0.1976,  0.1922,\n",
       "           0.1567,  0.1430,  0.2028,  0.1879]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0149,  0.1886,  0.1580,  0.1546,  0.1876,  0.1836,  0.1928,  0.1860,\n",
       "           0.1917,  0.2129,  0.2174,  0.2037],\n",
       "         [ 0.0121,  0.1563,  0.1684,  0.2058,  0.2032,  0.1784,  0.1818,  0.1727,\n",
       "           0.1450,  0.1659,  0.1860,  0.2066],\n",
       "         [-0.1616,  0.1961,  0.1929,  0.2170,  0.1764,  0.1775,  0.2004,  0.1866,\n",
       "           0.1996,  0.1820,  0.1747,  0.1733],\n",
       "         [-0.1035,  0.1828,  0.2067,  0.1825,  0.1806,  0.2077,  0.1933,  0.1896,\n",
       "           0.2045,  0.2127,  0.1601,  0.1875],\n",
       "         [ 0.0784,  0.1451,  0.1411,  0.1729,  0.1834,  0.2010,  0.1885,  0.1832,\n",
       "           0.1477,  0.1340,  0.1938,  0.1789]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0197,  0.1842,  0.1536,  0.1503,  0.1833,  0.1793,  0.1885,  0.1816,\n",
       "           0.1874,  0.2085,  0.2130,  0.1993],\n",
       "         [ 0.0297,  0.1738,  0.1858,  0.2233,  0.2206,  0.1958,  0.1991,  0.1900,\n",
       "           0.1624,  0.1832,  0.2033,  0.2240],\n",
       "         [-0.1464,  0.2103,  0.2070,  0.2310,  0.1907,  0.1918,  0.2145,  0.2008,\n",
       "           0.2138,  0.1961,  0.1888,  0.1874],\n",
       "         [-0.1017,  0.1839,  0.2079,  0.1836,  0.1816,  0.2088,  0.1944,  0.1908,\n",
       "           0.2055,  0.2137,  0.1611,  0.1886],\n",
       "         [ 0.0907,  0.1578,  0.1538,  0.1855,  0.1960,  0.2135,  0.2011,  0.1957,\n",
       "           0.1602,  0.1467,  0.2063,  0.1915]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0183,  0.1854,  0.1549,  0.1517,  0.1846,  0.1807,  0.1899,  0.1830,\n",
       "           0.1887,  0.2099,  0.2144,  0.2006],\n",
       "         [ 0.0417,  0.1844,  0.1962,  0.2337,  0.2310,  0.2062,  0.2094,  0.2003,\n",
       "           0.1727,  0.1935,  0.2137,  0.2343],\n",
       "         [-0.1355,  0.2213,  0.2181,  0.2421,  0.2018,  0.2030,  0.2256,  0.2119,\n",
       "           0.2248,  0.2071,  0.1998,  0.1985],\n",
       "         [-0.1160,  0.1713,  0.1953,  0.1710,  0.1690,  0.1962,  0.1819,  0.1783,\n",
       "           0.1930,  0.2012,  0.1486,  0.1760],\n",
       "         [ 0.0921,  0.1598,  0.1557,  0.1875,  0.1979,  0.2155,  0.2030,  0.1977,\n",
       "           0.1622,  0.1486,  0.2083,  0.1934]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0168,  0.1867,  0.1561,  0.1530,  0.1859,  0.1820,  0.1913,  0.1844,\n",
       "           0.1901,  0.2112,  0.2157,  0.2019],\n",
       "         [ 0.0403,  0.1839,  0.1957,  0.2332,  0.2306,  0.2057,  0.2090,  0.1998,\n",
       "           0.1723,  0.1931,  0.2132,  0.2339],\n",
       "         [-0.1451,  0.2122,  0.2091,  0.2331,  0.1928,  0.1940,  0.2167,  0.2029,\n",
       "           0.2159,  0.1982,  0.1908,  0.1894],\n",
       "         [-0.1039,  0.1820,  0.2059,  0.1817,  0.1797,  0.2069,  0.1925,  0.1889,\n",
       "           0.2036,  0.2118,  0.1593,  0.1866],\n",
       "         [ 0.0843,  0.1516,  0.1476,  0.1793,  0.1898,  0.2073,  0.1949,  0.1896,\n",
       "           0.1541,  0.1404,  0.2002,  0.1853]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0264,  0.1773,  0.1468,  0.1437,  0.1767,  0.1727,  0.1820,  0.1751,\n",
       "           0.1807,  0.2019,  0.2065,  0.1926],\n",
       "         [ 0.0459,  0.1884,  0.2001,  0.2376,  0.2349,  0.2101,  0.2133,  0.2042,\n",
       "           0.1766,  0.1973,  0.2175,  0.2381],\n",
       "         [-0.1506,  0.2071,  0.2040,  0.2280,  0.1876,  0.1889,  0.2116,  0.1977,\n",
       "           0.2107,  0.1931,  0.1857,  0.1843],\n",
       "         [-0.0917,  0.1938,  0.2177,  0.1935,  0.1915,  0.2186,  0.2043,  0.2006,\n",
       "           0.2154,  0.2236,  0.1711,  0.1984],\n",
       "         [ 0.0956,  0.1633,  0.1592,  0.1909,  0.2013,  0.2189,  0.2064,  0.2010,\n",
       "           0.1656,  0.1521,  0.2117,  0.1968]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0390,  0.1657,  0.1352,  0.1319,  0.1650,  0.1610,  0.1703,  0.1634,\n",
       "           0.1689,  0.1902,  0.1948,  0.1809],\n",
       "         [ 0.0417,  0.1843,  0.1960,  0.2335,  0.2309,  0.2060,  0.2093,  0.2002,\n",
       "           0.1726,  0.1933,  0.2134,  0.2340],\n",
       "         [-0.1551,  0.2027,  0.1996,  0.2236,  0.1832,  0.1844,  0.2071,  0.1933,\n",
       "           0.2063,  0.1886,  0.1813,  0.1799],\n",
       "         [-0.0971,  0.1894,  0.2133,  0.1890,  0.1871,  0.2142,  0.1999,  0.1963,\n",
       "           0.2110,  0.2192,  0.1666,  0.1940],\n",
       "         [ 0.1045,  0.1724,  0.1683,  0.2000,  0.2104,  0.2278,  0.2154,  0.2100,\n",
       "           0.1746,  0.1611,  0.2206,  0.2058]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0495,  0.1553,  0.1247,  0.1214,  0.1545,  0.1505,  0.1597,  0.1528,\n",
       "           0.1584,  0.1797,  0.1843,  0.1704],\n",
       "         [ 0.0152,  0.1587,  0.1705,  0.2081,  0.2056,  0.1808,  0.1841,  0.1751,\n",
       "           0.1473,  0.1680,  0.1881,  0.2087],\n",
       "         [-0.1460,  0.2113,  0.2081,  0.2321,  0.1918,  0.1930,  0.2156,  0.2018,\n",
       "           0.2148,  0.1971,  0.1898,  0.1884],\n",
       "         [-0.1094,  0.1775,  0.2015,  0.1772,  0.1752,  0.2024,  0.1881,  0.1845,\n",
       "           0.1992,  0.2074,  0.1548,  0.1821],\n",
       "         [ 0.1125,  0.1804,  0.1763,  0.2079,  0.2182,  0.2354,  0.2231,  0.2177,\n",
       "           0.1822,  0.1687,  0.2283,  0.2135]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0557,  0.1490,  0.1184,  0.1152,  0.1482,  0.1442,  0.1534,  0.1465,\n",
       "           0.1521,  0.1734,  0.1780,  0.1641],\n",
       "         [-0.0006,  0.1430,  0.1549,  0.1925,  0.1900,  0.1653,  0.1686,  0.1597,\n",
       "           0.1318,  0.1524,  0.1726,  0.1931],\n",
       "         [-0.1668,  0.1919,  0.1888,  0.2128,  0.1723,  0.1735,  0.1963,  0.1824,\n",
       "           0.1954,  0.1778,  0.1704,  0.1690],\n",
       "         [-0.1011,  0.1842,  0.2082,  0.1838,  0.1819,  0.2090,  0.1947,  0.1911,\n",
       "           0.2058,  0.2140,  0.1614,  0.1888],\n",
       "         [ 0.0963,  0.1647,  0.1606,  0.1923,  0.2027,  0.2201,  0.2077,  0.2023,\n",
       "           0.1669,  0.1534,  0.2129,  0.1981]], grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8d44c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = calculate_loss(decoder_outputs, decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "ec5b4916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b1b8e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(decoder_outputs, decoder_target):\n",
    "    \n",
    "    return F.cross_entropy(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        decoder_target.view(-1),\n",
    "        ignore_index=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9720f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, dropout_p = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_size = hidden_size, hidden_size = hidden_size, batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.first_decoder_input = torch.rand((BATCH_SIZE, 1, hidden_size))\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "      \n",
    "    def attn_scores(self, encoder_outputs, decoder_hidden):\n",
    "        return self.V(F.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden))).squeeze(-1)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor, target_lengths):\n",
    "        '''\n",
    "        encoder_hidden = [N, 1, H] \n",
    "        encoder_outputs = [N, L, H] \n",
    "        target_tensor = [N, L, 2]\n",
    "        target_lengths = [N, 1]\n",
    "        '''\n",
    "        \n",
    "        batch_size, max_len, _ = encoder_outputs.size()\n",
    "        encoder_outputs = torch.cat((self.first_decoder_input, encoder_outputs), dim=1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        self.first_decoder_input\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        for i in range(max_len + 1):\n",
    "            if i == 0:\n",
    "                decoder_input_embed = self.first_decoder_input\n",
    "            else:\n",
    "                decoder_input_embed = self.embedding(decoder_input)\n",
    "                \n",
    "            decoder_output, decoder_hidden = self.lstm(decoder_input_embed, decoder_hidden)\n",
    "            \n",
    "            '''decoder_output = [N, 1, H]'''\n",
    "            raw_attn_scores = self.attn_scores(encoder_outputs, decoder_output)\n",
    "            '''\n",
    "            \n",
    "            raw_attn_scores = [N, L+1]\n",
    "            mask attn_scores and softmax over only needed dictionary'''\n",
    "            \n",
    "            ones = torch.full(size = (batch_size, max_len + 1), fill_value = 1., dtype=torch.float)\n",
    "            #mask = ones.masked_fill(torch.cumsum(ones, dim=-1) > target_lengths + 1., float('-inf'))\n",
    "            \n",
    "            masked_attn_scores = raw_attn_scores.masked_fill(torch.cumsum(ones, dim=-1) > target_lengths + 1., float('-inf'))\n",
    "            masked_attn_softmax_scores = F.softmax(masked_attn_scores, dim=-1)\n",
    "            \n",
    "            decoder_outputs.append(masked_attn_softmax_scores)\n",
    "            \n",
    "            decoder_input = target_tensor[:, i-1, :].unsqueeze(1)\n",
    "            \n",
    "        decoder_outputs = torch.stack(decoder_outputs, dim=1)\n",
    "        return decoder_outputs\n",
    "        \n",
    "    \n",
    "    def calculate_loss(decoder_outputs, decoder_target):\n",
    "        loss = F.cross_entropy(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            decoder_target.view(-1),\n",
    "            ignore_index=-1)\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "decoder = Decoder(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809b487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
