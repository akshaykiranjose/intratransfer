{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6c9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch import optim \n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0160c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30593e",
   "metadata": {},
   "source": [
    "# TSP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e47cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "np.random.seed(0)\n",
    "\n",
    "def tsp_opt(points):\n",
    "    \"\"\"\n",
    "    Dynamic programing solution for TSP - O(2^n*n^2)\n",
    "    https://gist.github.com/mlalevic/6222750\n",
    "    :param points: List of (x, y) points\n",
    "    :return: Optimal solution\n",
    "    \"\"\"\n",
    "\n",
    "    def length(x_coord, y_coord):\n",
    "        return np.linalg.norm(np.asarray(x_coord) - np.asarray(y_coord))\n",
    "\n",
    "    # Calculate all lengths\n",
    "    all_distances = [[length(x, y) for y in points] for x in points]\n",
    "    # Initial value - just distance from 0 to every other point + keep the track of edges\n",
    "    A = {(frozenset([0, idx+1]), idx+1): (dist, [0, idx+1]) for idx, dist in enumerate(all_distances[0][1:])}\n",
    "    cnt = len(points)\n",
    "    for m in range(2, cnt):\n",
    "        B = {}\n",
    "        for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n",
    "            for j in S - {0}:\n",
    "                # This will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n",
    "                B[(S, j)] = min([(A[(S-{j}, k)][0] + all_distances[k][j], A[(S-{j}, k)][1] + [j])\n",
    "                                 for k in S if k != 0 and k != j])\n",
    "        A = B\n",
    "    res = min([(A[d][0] + all_distances[0][d[1]], A[d][1]) for d in iter(A)])\n",
    "    res[1].append(res[1][0])\n",
    "    return np.asarray(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43ff608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [04:27<00:00, 374.04it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "NUM_SAMPLES = 100000\n",
    "MAX_LENGTH = 7\n",
    "MIN_LENGTH = 7\n",
    "\n",
    "lengths = np.random.randint(MIN_LENGTH, MAX_LENGTH + 1, size=(NUM_SAMPLES))\n",
    "coordinates = [np.random.uniform(size=[l,2]) for l in lengths]\n",
    "solutions = [tsp_opt(points) for points in tqdm(coordinates)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67d84347",
   "metadata": {},
   "source": [
    "np.save('solutions_lite.npy', np.array(solutions, dtype=object))\n",
    "np.save('coordinates_lite.npy', np.array(coordinates, dtype=object))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77c67d12",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "coordinates = np.load('coordinates.npy', allow_pickle=True).tolist()\n",
    "solutions = np.load('solutions.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2651bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(coordinates, solutions, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3487d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).type(torch.FloatTensor).to(device), \\\n",
    "    torch.from_numpy(self.y[idx]).type(torch.LongTensor).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654cf3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TSPData(input_tensor_train, target_tensor_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03c16b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TSPData(input_tensor_val, target_tensor_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b44a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5770, 0.9926],\n",
      "         [0.2732, 0.5652],\n",
      "         [0.7807, 0.2254],\n",
      "         [0.5788, 0.3082],\n",
      "         [0.0294, 0.4081],\n",
      "         [0.3703, 0.6464],\n",
      "         [0.5723, 0.9969]],\n",
      "\n",
      "        [[0.4787, 0.9719],\n",
      "         [0.6227, 0.2497],\n",
      "         [0.6690, 0.0903],\n",
      "         [0.7040, 0.7991],\n",
      "         [0.5859, 0.2175],\n",
      "         [0.0724, 0.0400],\n",
      "         [0.7533, 0.4558]],\n",
      "\n",
      "        [[0.4553, 0.0151],\n",
      "         [0.6435, 0.0592],\n",
      "         [0.0685, 0.9878],\n",
      "         [0.0184, 0.8300],\n",
      "         [0.2522, 0.7085],\n",
      "         [0.4245, 0.0247],\n",
      "         [0.1513, 0.9368]],\n",
      "\n",
      "        [[0.4697, 0.6913],\n",
      "         [0.8001, 0.7468],\n",
      "         [0.3714, 0.7552],\n",
      "         [0.9143, 0.3212],\n",
      "         [0.0629, 0.4416],\n",
      "         [0.6047, 0.3109],\n",
      "         [0.6033, 0.9090]],\n",
      "\n",
      "        [[0.3453, 0.9383],\n",
      "         [0.6550, 0.0704],\n",
      "         [0.3712, 0.3764],\n",
      "         [0.4133, 0.0650],\n",
      "         [0.2065, 0.5442],\n",
      "         [0.3056, 0.0057],\n",
      "         [0.1013, 0.9888]],\n",
      "\n",
      "        [[0.8987, 0.0128],\n",
      "         [0.0792, 0.4951],\n",
      "         [0.7434, 0.2331],\n",
      "         [0.8231, 0.7566],\n",
      "         [0.7251, 0.9006],\n",
      "         [0.1335, 0.8736],\n",
      "         [0.7358, 0.4679]],\n",
      "\n",
      "        [[0.8451, 0.0795],\n",
      "         [0.3797, 0.9310],\n",
      "         [0.5441, 0.8208],\n",
      "         [0.5900, 0.0727],\n",
      "         [0.2436, 0.9583],\n",
      "         [0.4145, 0.4904],\n",
      "         [0.5361, 0.5901]],\n",
      "\n",
      "        [[0.8138, 0.5167],\n",
      "         [0.1895, 0.9102],\n",
      "         [0.9580, 0.0233],\n",
      "         [0.9816, 0.7224],\n",
      "         [0.5221, 0.1053],\n",
      "         [0.3167, 0.2233],\n",
      "         [0.3047, 0.9984]],\n",
      "\n",
      "        [[0.5558, 0.5435],\n",
      "         [0.6540, 0.9257],\n",
      "         [0.2776, 0.3295],\n",
      "         [0.0815, 0.1536],\n",
      "         [0.5486, 0.8989],\n",
      "         [0.3288, 0.0890],\n",
      "         [0.9964, 0.1327]],\n",
      "\n",
      "        [[0.6361, 0.7997],\n",
      "         [0.3467, 0.9065],\n",
      "         [0.0995, 0.4195],\n",
      "         [0.0642, 0.0224],\n",
      "         [0.9333, 0.6355],\n",
      "         [0.5827, 0.8016],\n",
      "         [0.0243, 0.0585]],\n",
      "\n",
      "        [[0.1621, 0.9259],\n",
      "         [0.1777, 0.7798],\n",
      "         [0.1314, 0.5493],\n",
      "         [0.5764, 0.2906],\n",
      "         [0.8071, 0.0647],\n",
      "         [0.7105, 0.6468],\n",
      "         [0.9367, 0.4851]],\n",
      "\n",
      "        [[0.7106, 0.8763],\n",
      "         [0.9568, 0.6981],\n",
      "         [0.3089, 0.2052],\n",
      "         [0.0130, 0.2551],\n",
      "         [0.3111, 0.4926],\n",
      "         [0.7734, 0.5077],\n",
      "         [0.6538, 0.1501]],\n",
      "\n",
      "        [[0.1401, 0.2564],\n",
      "         [0.3981, 0.8469],\n",
      "         [0.6105, 0.7454],\n",
      "         [0.0217, 0.6195],\n",
      "         [0.5630, 0.2945],\n",
      "         [0.0412, 0.0873],\n",
      "         [0.4718, 0.9460]],\n",
      "\n",
      "        [[0.0016, 0.3610],\n",
      "         [0.5524, 0.6157],\n",
      "         [0.2783, 0.8113],\n",
      "         [0.7744, 0.6938],\n",
      "         [0.6600, 0.2913],\n",
      "         [0.1234, 0.1607],\n",
      "         [0.0539, 0.2970]],\n",
      "\n",
      "        [[0.7921, 0.9788],\n",
      "         [0.1893, 0.6623],\n",
      "         [0.0927, 0.4317],\n",
      "         [0.5132, 0.7149],\n",
      "         [0.7058, 0.5167],\n",
      "         [0.6635, 0.4174],\n",
      "         [0.6281, 0.1137]],\n",
      "\n",
      "        [[0.6654, 0.9104],\n",
      "         [0.5033, 0.7925],\n",
      "         [0.0826, 0.9516],\n",
      "         [0.8359, 0.0817],\n",
      "         [0.9520, 0.4565],\n",
      "         [0.7287, 0.9632],\n",
      "         [0.8681, 0.1620]]], device='cuda:0') tensor([[0, 2, 3, 4, 1, 5, 6, 0],\n",
      "        [0, 3, 6, 1, 4, 2, 5, 0],\n",
      "        [0, 1, 4, 6, 2, 3, 5, 0],\n",
      "        [0, 2, 4, 5, 3, 1, 6, 0],\n",
      "        [0, 2, 1, 3, 5, 4, 6, 0],\n",
      "        [0, 1, 5, 4, 3, 6, 2, 0],\n",
      "        [0, 3, 5, 4, 1, 2, 6, 0],\n",
      "        [0, 2, 4, 5, 1, 6, 3, 0],\n",
      "        [0, 2, 3, 5, 6, 1, 4, 0],\n",
      "        [0, 5, 1, 2, 6, 3, 4, 0],\n",
      "        [0, 1, 2, 3, 4, 6, 5, 0],\n",
      "        [0, 1, 5, 6, 2, 3, 4, 0],\n",
      "        [0, 3, 1, 6, 2, 4, 5, 0],\n",
      "        [0, 2, 1, 3, 4, 5, 6, 0],\n",
      "        [0, 3, 1, 2, 6, 5, 4, 0],\n",
      "        [0, 5, 4, 6, 3, 2, 1, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_loader)\n",
    "for x,y in it:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090be67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 7, 2]), torch.Size([16, 8]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d109469",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34dabd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_dim, embedding_dim, is_bidir):\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.is_bidir = is_bidir\n",
    "        self.embedding = nn.Linear(2, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, encoder_dim, bidirectional = is_bidir, batch_first = True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        b_sz = inputs.size(0)\n",
    "        x = self.embedding(inputs)\n",
    "        self.h_0, self.c_0 = self.init_hidden(b_sz), self.init_hidden(b_sz)\n",
    "        outputs, states = self.lstm(x, (self.h_0, self.c_0))\n",
    "        return outputs, states  #make batch_first\n",
    "        \n",
    "    def init_hidden(self, b_sz):\n",
    "        D = 2 if self.is_bidir else 1\n",
    "        return torch.zeros(D, b_sz, self.encoder_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c140391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256\n",
    "EMBEDS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b24c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(encoder_dim = UNITS, \n",
    "                  embedding_dim = EMBEDS, \n",
    "                  is_bidir = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f49b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_dim, encoder_dim, embedding_dim, is_enc_bidir = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.decoder_dim = 2*encoder_dim if is_enc_bidir else encoder_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Linear(2, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.decoder_dim, batch_first = True)\n",
    "              \n",
    "        W1_indim = 2*self.encoder_dim if is_enc_bidir == True else self.encoder_dim\n",
    "        self.W1 = nn.Linear(W1_indim, self.decoder_dim, bias=False) \n",
    "        self.W2 = nn.Linear(self.decoder_dim, self.decoder_dim, bias=False)\n",
    "        self.V = nn.Linear(self.decoder_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x, states, encoder_outputs):\n",
    "        \"x: [1, 2]: coordinate for point 1, batched\"\n",
    "        \"prev_hidden: encoder_hidden\"\n",
    "        b_sz = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        _, dec_states = self.lstm(x, states)\n",
    "        \n",
    "        score = torch.tanh(self.W1(encoder_outputs) + self.W2(dec_states[0].permute(1,0,2)))\n",
    "        \n",
    "        output_probs = self.V(score).view(b_sz, -1)\n",
    "        \n",
    "        return output_probs, dec_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1bf72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(decoder_dim = UNITS, \n",
    "                  encoder_dim = UNITS, \n",
    "                  embedding_dim = EMBEDS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3e126",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e62671",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),\n",
    "                       lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "loss_epochs = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), position=0, desc='EPOCHS'):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "    \n",
    "        b_sz = X.size(0)\n",
    "        loss = 0\n",
    "        enc_hidden_all, dec_hidden = encoder(X)\n",
    "        dec_hidden = [x.permute(1,0,2).reshape(1, b_sz, -1) for x in dec_hidden]\n",
    "        dec_input = X[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, y.size(1)):\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_hidden_all)\n",
    "            loss += loss_fn(predictions, y[:, 1])\n",
    "            if np.random.rand() > 0.999:\n",
    "                #use predicted best\n",
    "                pred_token = torch.argmax(torch.nn.functional.softmax(predictions, dim=-1), dim=-1)\n",
    "                dec_input = X[torch.arange(b_sz), pred_token, :].unsqueeze(1)\n",
    "            else:\n",
    "                dec_input = X[torch.arange(b_sz), y.squeeze()[:, t], :].unsqueeze(1) # teacher forcing\n",
    "\n",
    "        batch_loss = (loss.item() / int(y.size(1)))\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_epochs.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2576f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME_1 = \"encoder.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME_1\n",
    "\n",
    "torch.save(obj=encoder.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)\n",
    "\n",
    "MODEL_NAME_2 = \"decoder.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME_2\n",
    "\n",
    "torch.save(obj=decoder.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c55854",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME_1\n",
    "encoder.load_state_dict(torch.load(f = MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME_2\n",
    "decoder.load_state_dict(torch.load(f = MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f634c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for X, y in val_loader:\n",
    "    with torch.no_grad():\n",
    "        b_sz = X.size(0)\n",
    "        enc_hidden_all, enc_hidden_last = encoder(X)\n",
    "        dec_hidden = [x.permute(1,0,2).reshape(1, b_sz, -1) for x in enc_hidden_last]\n",
    "        dec_input = X[:, 0].unsqueeze(1)\n",
    "        tour = []\n",
    "        pred_token = None\n",
    "        while pred_token != 0:\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_hidden_all)\n",
    "            pred_token = torch.argmax(torch.nn.functional.softmax(predictions, dim=-1), dim=-1)\n",
    "            tour.append(pred_token.item())\n",
    "            print(pred_token.item())\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02df899a",
   "metadata": {},
   "source": [
    "what was the cause of the error?\n",
    "what was done to correct it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aef9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input.shape, dec_hidden[0].shape, dec_hidden[1].shape, enc_hidden_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f220bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
